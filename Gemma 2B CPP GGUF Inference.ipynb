{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ZSixzGIrCncfPxXtaRDvmaj4_U9Eq-3D",
      "authorship_tag": "ABX9TyOLkiTo9IhBeUhOuIkrN6zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3030182da849436ea8fc7277739a7a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66cbb1a2853649febe11282e35b3d016",
              "IPY_MODEL_9ca2d30dc8414ddf894c114fe4a3c1f0",
              "IPY_MODEL_b2f16cfe163a4a4b815568d5533977c2"
            ],
            "layout": "IPY_MODEL_f9a570f0f9fb4d17aac08249d189123b"
          }
        },
        "66cbb1a2853649febe11282e35b3d016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fa65f37f0f54b798e1ea9fd86d583b0",
            "placeholder": "​",
            "style": "IPY_MODEL_c88681bd53634646829e80f6dd3ddae2",
            "value": "gemma-2b-it.gguf: 100%"
          }
        },
        "9ca2d30dc8414ddf894c114fe4a3c1f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd32b64735f24b82a826fe6fba76e496",
            "max": 10031780704,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_718e39159c5d4a6d8686fc90a3cc339c",
            "value": 10031780704
          }
        },
        "b2f16cfe163a4a4b815568d5533977c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e5981090aca4161ae99522c835bbe95",
            "placeholder": "​",
            "style": "IPY_MODEL_b5fa5f2c6a0b45a9ab0662f409ed23ba",
            "value": " 10.0G/10.0G [01:15&lt;00:00, 204MB/s]"
          }
        },
        "f9a570f0f9fb4d17aac08249d189123b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa65f37f0f54b798e1ea9fd86d583b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c88681bd53634646829e80f6dd3ddae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd32b64735f24b82a826fe6fba76e496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718e39159c5d4a6d8686fc90a3cc339c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e5981090aca4161ae99522c835bbe95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5fa5f2c6a0b45a9ab0662f409ed23ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpanSM/LLM-Notebooks/blob/main/Gemma%202B%20CPP%20GGUF%20Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j1jLzpDa5N_"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub scikit-build-core==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation"
      ],
      "metadata": {
        "id": "KDCf9KUsc58R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "PsgsDuZabqww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token \"hf_xxxx\""
      ],
      "metadata": {
        "id": "1TEHPHQJejSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-2b-it\"\n",
        "model_file = \"gemma-2b-it.gguf\"\n",
        "model_path = hf_hub_download(model_name,\n",
        "                             filename=model_file,\n",
        "                             local_dir='/content',\n",
        "                             token=HF_TOKEN)\n",
        "print(\"My model path: \", model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "3030182da849436ea8fc7277739a7a9d",
            "66cbb1a2853649febe11282e35b3d016",
            "9ca2d30dc8414ddf894c114fe4a3c1f0",
            "b2f16cfe163a4a4b815568d5533977c2",
            "f9a570f0f9fb4d17aac08249d189123b",
            "5fa65f37f0f54b798e1ea9fd86d583b0",
            "c88681bd53634646829e80f6dd3ddae2",
            "cd32b64735f24b82a826fe6fba76e496",
            "718e39159c5d4a6d8686fc90a3cc339c",
            "4e5981090aca4161ae99522c835bbe95",
            "b5fa5f2c6a0b45a9ab0662f409ed23ba"
          ]
        },
        "id": "0X5OmVmqedZx",
        "outputId": "4962017b-7c9c-44d5-f450-3788170f790a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-2b-it.gguf:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3030182da849436ea8fc7277739a7a9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model path:  /content/gemma-2b-it.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/gemma-2b-it.gguf'"
      ],
      "metadata": {
        "id": "T4fcLKZDij-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=model_path, n_gpu_layers=-1, n_ctx =8192)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICPLrl3GeopN",
        "outputId": "aecb3b13-0792-44a3-c212-80ce6992c668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /content/gemma-2b-it.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
            "llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n",
            "llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n",
            "llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n",
            "llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n",
            "llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - type  f32:  164 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256128\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2048\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 1\n",
            "llm_load_print_meta: n_layer          = 18\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 8\n",
            "llm_load_print_meta: n_embd_k_gqa     = 256\n",
            "llm_load_print_meta: n_embd_v_gqa     = 256\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 16384\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = all F32 (guessed)\n",
            "llm_load_print_meta: model params     = 2.51 B\n",
            "llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n",
            "llm_load_print_meta: general.name     = gemma-2b-it\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 19/19 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2001.00 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  9561.29 MiB\n",
            ".............................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   144.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   504.25 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    20.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 601\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(\"State a quote from famous person and explain the meaning.\", max_tokens=1000, temperature = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10mleFutjl9v",
        "outputId": "6dfc14c8-d8e8-4671-e340-a894625a294f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     233.09 ms\n",
            "llama_print_timings:      sample time =     285.45 ms /    70 runs   (    4.08 ms per token,   245.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     232.95 ms /    12 tokens (   19.41 ms per token,    51.51 tokens per second)\n",
            "llama_print_timings:        eval time =    2893.77 ms /    69 runs   (   41.94 ms per token,    23.84 tokens per second)\n",
            "llama_print_timings:       total time =    5029.23 ms /    81 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-2tCwwYofSt",
        "outputId": "6fa34bf9-44fc-438a-94ed-4351dfd6a258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"The only true wisdom is in knowing you know nothing.\" - Socrates\n",
            "\n",
            "**Meaning:** This quote means that the greatest way to learn is to accept that we don't know everything and that continuous growth and exploration are essential for personal development. It encourages us to embrace a lifelong learning mindset and to be open to new experiences and perspectives.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "response = llm(\"State a quote from famous person and explain the meaning.\", max_tokens=1000, temperature = 0)\n",
        "# 4.61 s ± 179 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Zj6UPufZPy",
        "outputId": "20f17bc1-ccd1-4cc4-c9a9-4fcfca736839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     307.69 ms /    70 runs   (    4.40 ms per token,   227.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2923.37 ms /    70 runs   (   41.76 ms per token,    23.94 tokens per second)\n",
            "llama_print_timings:       total time =    5051.07 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     271.12 ms /    70 runs   (    3.87 ms per token,   258.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2895.97 ms /    70 runs   (   41.37 ms per token,    24.17 tokens per second)\n",
            "llama_print_timings:       total time =    4445.11 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     279.79 ms /    70 runs   (    4.00 ms per token,   250.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2896.35 ms /    70 runs   (   41.38 ms per token,    24.17 tokens per second)\n",
            "llama_print_timings:       total time =    4478.77 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     311.18 ms /    70 runs   (    4.45 ms per token,   224.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2909.56 ms /    70 runs   (   41.57 ms per token,    24.06 tokens per second)\n",
            "llama_print_timings:       total time =    4900.56 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     274.06 ms /    70 runs   (    3.92 ms per token,   255.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2904.73 ms /    70 runs   (   41.50 ms per token,    24.10 tokens per second)\n",
            "llama_print_timings:       total time =    4510.34 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     304.59 ms /    70 runs   (    4.35 ms per token,   229.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2907.14 ms /    70 runs   (   41.53 ms per token,    24.08 tokens per second)\n",
            "llama_print_timings:       total time =    4849.91 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     278.43 ms /    70 runs   (    3.98 ms per token,   251.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2901.77 ms /    70 runs   (   41.45 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =    4539.69 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    2035.84 ms\n",
            "llama_print_timings:      sample time =     277.58 ms /    70 runs   (    3.97 ms per token,   252.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =    2897.04 ms /    70 runs   (   41.39 ms per token,    24.16 tokens per second)\n",
            "llama_print_timings:       total time =    4458.58 ms /    71 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.61 s ± 179 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def llama_cpp_predict(prompt):\n",
        "  response = llm(\n",
        "    prompt,\n",
        "    max_tokens=1000,\n",
        "    temperature = 0.1\n",
        "  )\n",
        "  return response['choices'][0]['text'].strip()\n",
        "\n",
        "def format_prompt(prompt, variable_dict):\n",
        "    for k,v in variable_dict.items():\n",
        "        prompt = prompt.replace('{'+k+'}', v)\n",
        "    return prompt.strip()"
      ],
      "metadata": {
        "id": "XY8SjS1qoUoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('default/classes/AccountFeatureHelper.cls', 'r') as f:\n",
        "    code_file = f.read()\n",
        "\n",
        "__code_description_prompt = \"\"\"\n",
        "Extract the following information from the given Java code and return it in natural language format:\n",
        "\n",
        "1. Class Descriptions:\n",
        "   - Class Name: Identify the purpose of the class.\n",
        "   - Class Role: Describe the role of the class in the overall system.\n",
        "   - Inheritance: Note any inheritance relationships (superclasses and interfaces implemented).\n",
        "\n",
        "2. Method Descriptions:\n",
        "   - Method Name: State what each method does.\n",
        "   - Parameters: List and describe the parameters each method accepts.\n",
        "   - Return Type: Describe what each method returns.\n",
        "   - Functionality: Explain the main functionality of the method.\n",
        "   - Exceptions: Mention any exceptions that the method might throw and under what circumstances.\n",
        "\n",
        "3. Attributes/Fields:\n",
        "   - Field Name: Describe the purpose of each field.\n",
        "   - Type: Note the data type of each field.\n",
        "   - Usage: Explain how and where the field is used within the class.\n",
        "\n",
        "4. Relationships:\n",
        "   - Interactions: Describe how the class and methods interact with each other.\n",
        "   - Dependencies: Note any dependencies on other classes or external libraries.\n",
        "\n",
        "5. Logic Flow:\n",
        "   - Control Structures: Describe the use of control structures like loops and conditionals.\n",
        "   - Key Algorithms: Summarize any important algorithms used in the code.\n",
        "\n",
        "Java Code:\n",
        "{java_code}\n",
        "\n",
        "Description:\n",
        "\"\"\"\n",
        "\n",
        "cd_prompt = format_prompt(__code_description_prompt, {'java_code': code_file})\n",
        "response = llama_cpp_predict(cd_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFcFQdmToA_o",
        "outputId": "71bb9cbc-6bfc-4487-a3ea-329495ae0fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     233.09 ms\n",
            "llama_print_timings:      sample time =    1850.92 ms /   468 runs   (    3.95 ms per token,   252.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_print_timings:        eval time =   20942.65 ms /   468 runs   (   44.75 ms per token,    22.35 tokens per second)\n",
            "llama_print_timings:       total time =   33978.72 ms /   469 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This Java code defines a class called `AccountFeatureHelper` that provides methods and fields for handling data related to accounts and features. \n",
            "\n",
            "The class has the following methods:\n",
            "\n",
            "* `getRecordsData`: This method takes a JSON string as input and returns a RecordResponse object containing records filtered based on the provided parameters.\n",
            "* `getOptions`: This method takes a list of product IDs and field names as input and returns a List of ProductOptionResponse objects, each representing an option for a specific product feature.\n",
            "* `getChildRelationShipName`: This method takes two object names as input and returns the name of the child relationship between them.\n",
            "\n",
            "The class also has the following fields:\n",
            "\n",
            "* `fieldnames`: A list of field names to be retrieved from the database.\n",
            "* `objectType`: The type of the object being accessed.\n",
            "* `filters`: A list of filters to apply to the query.\n",
            "* `staticFilters`: A list of static filters to apply to the query.\n",
            "* `primarySortField` and `primarySortOrder`: The field to sort by and in what order.\n",
            "* `secondarySortField` and `secondarysortOrder`: The field to sort by in addition to the primary sort field.\n",
            "* `searchText`: The search text to perform on the results.\n",
            "* `searchField`: The field to use for searching.\n",
            "* `summaryField`: The field to use for summary calculations.\n",
            "* `lineItemObject` and `lineItemObjectRelation`: Objects related to the line item object.\n",
            "* `accountFieldToFilter`: The field to filter by for account-related data.\n",
            "* `accountId`: The ID of the account.\n",
            "* `relationshipFieldName`: The name of the relationship field.\n",
            "* `relationshipFieldValue`: The value of the relationship field.\n",
            "* `templateRecordId`: The ID of the template record.\n",
            "* `rowLimit` and `rowOffSet`: Limits on the number of rows to be retrieved.\n",
            "Page name for pagination.\n",
            "* `aggregateFunction`: The function to aggregate the results.\n",
            "* `aggregateField`: The field to aggregate the results by.\n",
            "* `isListPage`: A flag indicating if the result set is paginated.\n",
            "* `productIb`: A list of product IDs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmXaPZXAqRd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}